# L24 (November 19th, 2019) - Large State Spaces

- know logic behind Bellman Equation (should be able to use it / derive it from memory)
- same with prob equations...

- Nothing we have done so far is implementable in a realistic situation so far :(
- we have assumed that the underlying structures are exactly representably (transition, reward, etc...)

## Function Approximation
- alternative to represent functions in a parametric form (eg: using linear functions)
- <code>Q(s,a)=SUM<sub>i</sub>w<sub>i</sub>f<sub>i</sub>(s,a)</code>
- each <code>f<sub>i</sub>(s,a)</code> is a feature of a state and action pair
- consequences
	- (+) huge storage savings
	- (+) b/c states&actions are represented via features, provides generalization
		- (+) connected to Machine Learning
	- (-) leads to *aliasing* (POMDPs)
	- (-) convergence guarantees may be lost
	- need careful feature design for effective learning.
- Note
	- can easily build a linear representation of the value functions
		- assign each state an indicator feature
		- shouldn't do it coach
		- end up with # of features = # of states * # of actions
- Q-learning w/ linear func. aprox.
	- suppose <code>Q<sub>w</sub>(s,a)=SUM<sub>i</sub>w<sub>i</sub>f<sub>i</sub>(s,a)</code>
	- how does q-learning work
		- update w<sub>i</sub>'s weight to change the Q func
	- suppose we observe (s,a,s')
	- for this transition we have a "target Q-Value":
		- GET_EQU_SLIDE_14
	- and our current value:
		<code>Q<sub>w</sub>(s,a)=SUM<sub>i</sub>w<sub>i</sub>f<sub>i</sub>(s,a)</code>
	- Temporal Difference Loss Function:
		- GET_EQU_SLIDE_15
	- then if we min this loss function w.r.t. **w**, we will find the **w** we need
	- use calculus to smooth the function
		- GET_EQU_SLIDE_16
- Updated Q-Learning w/ Linear Function Approximation
	- GET_EQU_SLIDE_17
	- "does not quite work"
	- how do you check convergence???
		- cant
- checking convergence
	- run algorithm for set number of iterations then stop
	- stop when the value of the best greedy policy has been stable for a while