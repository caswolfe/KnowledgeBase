# L26 (December 3d, 2019) - Theory of Reinforcement Learning

- Key Questions
	- do they always converge to the optimal policy?
	- how long do they take?
	- if they don't converge, can we characterize what they do?
	- what are some cases where they don't converge?

# Convergence of Value Iteration
- we shall show that the value iteration algo allways converge to the optimal policy
	- for simplification, we assume that rewards are associated with states, rather than state-action pairs
- GET_VISUAL_SLIDE_5
- "Bellman Backup Operator" <code>S</code>
	- GET_EQU_SLIDE_6
- this operator performs on step of value iteration each time it is applied
- we show that the operator has some interesting properties
- Properties:
	- Monotonicity
		- GET_EQU_SLIDE_7
	- Exponential Decay of Constants
		- GET_VISUAL_SLIDE_8
	- Infinity Norm
		- GET_VISUAL_SLIDE_9
- Contraction Theorem
	- GET_VISUAL_SLIDE_10
- Convergence Theorem
	- GET_VISUAL_SLIDE_13
- Q-Learning