# L25 (November 26th, 2019) - Large State-Action Spaces Cont. & Multiagent RL

- Evaluating RL Algorithms
	- suppose we want to measure the performance of an RL algorithm (ex: q-learning)
	- as algo learns, we will "freeze" every so often
	- for a while, we'll just execute the greedy policy and track the cumulative rewards
	- learning curve:
		- GET_GRAPH_SLIDE_4
- Episodic Problems
	- in some RL problems, has a "terminal" or "absorbing" state
		- "episodic" problems
		- most games like this
		- put "# of episodes" for x-axis of graph
- Q-Learning w/ **Nonlinear** Function approximation
	- start w/ arbitrary **w**. This defines an initial Q<sub>0</sub>
	- f/ each greedy policy &pi; with GLIE exploration
	- for each observed (s,a,s') do
		- GET_EQU_SLIDE_6
	- until stopping criterion met

## Multiagent RL

- what happens if there are multiple agents in the world?
	- what if they're acting simultaneously?
	- simultaneous learning???
- settings
	- collaborative
		- agents try to jointly optimize some utility function
		- can generalize framework(MDP) to the joint problems
	- competitive
		- agents optimize their own utility functions, and typically maximizing one function means minimizing some other function
		- Formal setting: Game Theory
		- Generally have some element of partial observability
- collaborative RL
	- multiagent MDP ("MAMDP")
	- joint states and actions: **s** from S<sup>N</sup>, **a** from A<sup>N</sup>
	- joint transitions and reward function: <code>R(**s**,**a**)</code>, <code>T(**s**,**a**,**s'**)</code>
	- can define Bellman optimality in the same way as single agent b/c using multiagent value/action-value functions <code>V(**s**)</code> and <code>Q(**s**,**a**)</code>
- central vs decentralized control
	- previous setup is an example of "central control", where we imagine each agent communicating everything ack to one place and getting actions back
	- Alternative: decentralized control, where each agent:
		- possibly senses the state independently
		- possible acts independently
- centralized vs decentralized
	- centralized
		- can be formally modeled, have optimality guarantees
		- difficult to scale w/ number of agents
	- decentralized controllers:
		- scale well w/ agents
		- generally lack optimality guarantees (partial observability)
		- communication may be a factor
	- in both cases, there are robustness issues
- scalability issues in central control
	- representing the value function
		- handled via func aprox
		- value func sharing
		- roles
	- selecting actions
		- complex, possibly nonlinear optimization problem
- coordination graphs (Guestrin et al ICML 2002)
	- key insight: generally, most agents do not need to coordinate with most other agents
	- suppose we create a graph that captures who needs to coordinate with who
		- then solve for a policy using this structure
	- we can treat this as a graphical modeled
		- probabilistic inference <--> action selection!
- coordination graphs (structure)
	- suppose there are 4 agents and in some task, 1&2, 2&4, 4&3, and 3&1 need to coordinate
	- GET_FIGURE_SLIDE_14
	- Q function on each node, inference done on edges rather than in the nodes
	- Q func is a sum of "edge potentials" (the edge Q functions)
- Finding the greedy policy
	- "agent" elimination (variable elimination w/ agents)
	- GET_EQU_SLIDE_15
	- can employ heuristics to optimize the variable elimination ordering
- selecting actions
	- back-trace the agent elimination to find the optimal action
	- no additional computation required
- Coordinated Q-Learning with Function Approximation
	- start w/ arbitrary **w**. This defines an initial Q<sub>0</sub>
	- Follow greedy *joint* policy &pi; with GLIE exploration
	- for each observed (s,a,s') do
		- GET_EQU_SLIDE_19
	- until iteration bound met or reward stabilizes