# Lecture #22 (November 12th, 2019) - Exploration Exploitation Trade-off

## Reinforcement Learning

- **Passive Reinforcement Learning**
	- given a policy &pi;, compute the value function
	- what if we don't have the transition and reward functions?
	- Adaptive Dynamic Programming
		- "running the policy"
		- as you run the policy, record rewards from state and action pairs.
		- can use to create approximations of T&R (transition and reward function)
		- can then use these approximation to solve the bellman equation and therefore solve for the value function.
		- **GET_EQU_SLIDE_5**
		- <code>s<sup>2</sup>a</code> is the size of the model for the transition model
	- Model-free Passive RL Learning
		- Adaptive DP is a "model-based" algo
			- requires us to estimate T&R in order to work.
		- Don't need this, can compute the value function w/o estimating T&R
			- aka "model-free" methods
	- Passive RL w/ Monte Carlo
		- we have a fixed policy & want to compute its value, but we don't want to store/estimate T&R
		- Brute force, sampling trajectories from each state and summing the result for that state.
		- **GET_EQU_SLIDE_8**
		- note the absence of the Bellman Equation
		- converges to the correct solution
		- each state estimate is preformed independently of other states, despite the fact that they are related.
	- RL w/ MC/DP combination
		- "temporal difference learning"
			- family of algo's which combinde MC&DP methods
			1) start w/ arbitrary value function V<sub>0</sub>
			1) run the given policy
				1) for each observed <code>(s, &pi;(s), s')</code>
					- **GET_EQU_SLIDE_11**
				1) until <code>ABS[V<sub>i+i</sub><sup>&pi;</sup>(s) - V<sub>i</sub><sup>&pi;</sup>(s)]</code> is very small for all s
			- note &alpha;, the learning rate
- **Active Reinforcement Learning**
	- find the optimal policy
	- still don't know T&R
	- note: *active vs passive learning is not a literature term, only used to better split concepts*
	- **Model Based**
			1) start w/ arbitrary policy
			1) as agent executes, record T&R
			1) use records to approximate T&R
			1) can use approximations in the value-iteration algorithm
		- doesn't work...
		- arbitrary policy affects the transition distribution :(
		- no exploration
	- Exploration Problem
		- agent does not try to improve the init policy, because it doesn't know that there are other actions.
		- fix: allow agent to go rogue

## Exploration Strategy

- &epsilon;-greed exploration
	- in each iteration, agent will
		1) follow action recommended by policy ("greedy action") w/ prob <code>1-&epsilon;</code>
		1) execute a random action w/ prob &epsilon;
		1) decay &epsilon; over time
	- "greedy" - pick the best action according to the current policy/value function
- Optimistic Exploration
	- when performing value iteration, for any <code>R(s,a)</code> not seen yet, use a large positive quantity R<sub>MAX</sub>
		- encourages the agent to try unseen actions
- Boltzmann Exploration
	- put a prob dist on all actions based on their value, then sample from that dist
	- **GET_EQU_SLIDE_20**
	- larger temp mean numerator ~1, Pr(a) ~1, being normalized. (random picking when temp high)
	- small temp allows for true probabilities to energy, returning greedy solutions rather than random.
	- might want to normalize <code>Q(a)</code> when implementing to prevent overflow, also don't overload T either...
	- akin to simulated annealing
- **GLIE Exploration**
	- Greedy in the Limit of Infinite Exploration
	- it can be shown that any GLIE exploration strat will eventually converge to the optimal policy as required.
- **Active ADP Redux**
	1) start w/ random policy
	1) as the agent executes the policy **w/ exploration**, keep tract of transitions and rewards
	1) @ each step, approximate T&R
	1) can use the approximations in the *value iteration* algorithm